{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpIGEC6Ur06TSJHEHAvKVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmm0403/TIL/blob/main/221109_trans_1hour_sample37.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1RTG2z_ollEu"
      },
      "outputs": [],
      "source": [
        "#Import the necessary data science libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Importing our TensorFlow libraries\n",
        "from math import sqrt\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from matplotlib import pyplot\n",
        "import sklearn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg"
      ],
      "metadata": {
        "id": "Uu_x_SvelrrD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the data set as a pandas DataFrame\n",
        "df = pd.read_csv('입력데이터.csv', encoding='cp949')"
      ],
      "metadata": {
        "id": "iFybAlYflv_c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the data set as a pandas DataFrame\n",
        "df1 = pd.read_csv('라벨데이터37816.csv', encoding='cp949')"
      ],
      "metadata": {
        "id": "aev5l932mIcG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_index = df[(df[\"날짜\"].between('2018-02-03','2018-02-10'))].index\n",
        "df = df.drop(df_index)\n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "gvIzBHyvmMfO",
        "outputId": "cac3c029-daf8-421d-b77e-c1cb6048b7b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              날짜       시:분  온도 (℃)  습도 (%)  CO2 (ppm)  일사 (w/㎡)\n",
              "0     2017-10-20   0:15:00    18.5    90.7      651.0       6.8\n",
              "1     2017-10-20   1:15:00    18.2    90.4      675.5       6.8\n",
              "2     2017-10-20   2:15:00    17.6    89.8      685.5       7.3\n",
              "3     2017-10-20   3:15:00    17.2    90.5      722.5       6.8\n",
              "4     2017-10-20   4:15:00    18.1    90.8      732.5       7.3\n",
              "...          ...       ...     ...     ...        ...       ...\n",
              "2539  2018-02-02  19:15:00    14.8    87.0      482.5       7.1\n",
              "2540  2018-02-02  20:15:00    15.5    85.3      492.5       7.1\n",
              "2541  2018-02-02  21:15:00    15.3    84.4      515.5       7.3\n",
              "2542  2018-02-02  22:15:00    16.0    83.4      542.0       7.3\n",
              "2543  2018-02-02  23:15:00    15.9    82.8      562.5       7.3\n",
              "\n",
              "[2544 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d5cd0fb-d047-49e9-a4c8-ff42f0842de5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>날짜</th>\n",
              "      <th>시:분</th>\n",
              "      <th>온도 (℃)</th>\n",
              "      <th>습도 (%)</th>\n",
              "      <th>CO2 (ppm)</th>\n",
              "      <th>일사 (w/㎡)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-10-20</td>\n",
              "      <td>0:15:00</td>\n",
              "      <td>18.5</td>\n",
              "      <td>90.7</td>\n",
              "      <td>651.0</td>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-10-20</td>\n",
              "      <td>1:15:00</td>\n",
              "      <td>18.2</td>\n",
              "      <td>90.4</td>\n",
              "      <td>675.5</td>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-10-20</td>\n",
              "      <td>2:15:00</td>\n",
              "      <td>17.6</td>\n",
              "      <td>89.8</td>\n",
              "      <td>685.5</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-10-20</td>\n",
              "      <td>3:15:00</td>\n",
              "      <td>17.2</td>\n",
              "      <td>90.5</td>\n",
              "      <td>722.5</td>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-10-20</td>\n",
              "      <td>4:15:00</td>\n",
              "      <td>18.1</td>\n",
              "      <td>90.8</td>\n",
              "      <td>732.5</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2539</th>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>19:15:00</td>\n",
              "      <td>14.8</td>\n",
              "      <td>87.0</td>\n",
              "      <td>482.5</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2540</th>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>20:15:00</td>\n",
              "      <td>15.5</td>\n",
              "      <td>85.3</td>\n",
              "      <td>492.5</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2541</th>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>21:15:00</td>\n",
              "      <td>15.3</td>\n",
              "      <td>84.4</td>\n",
              "      <td>515.5</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2542</th>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>22:15:00</td>\n",
              "      <td>16.0</td>\n",
              "      <td>83.4</td>\n",
              "      <td>542.0</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2543</th>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>23:15:00</td>\n",
              "      <td>15.9</td>\n",
              "      <td>82.8</td>\n",
              "      <td>562.5</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2544 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d5cd0fb-d047-49e9-a4c8-ff42f0842de5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4d5cd0fb-d047-49e9-a4c8-ff42f0842de5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4d5cd0fb-d047-49e9-a4c8-ff42f0842de5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.DataFrame()\n",
        "for i in range(4):\n",
        "  df2 = df2.append(df)\n",
        "df2 = df2.reset_index(drop=True)\n",
        "df2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "KIm5LvzumNZC",
        "outputId": "9e5b3f28-ee31-4c14-c771-415a34f9a75d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-c025cffaa333>:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df2 = df2.append(df)\n",
            "<ipython-input-6-c025cffaa333>:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df2 = df2.append(df)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               날짜       시:분  온도 (℃)  습도 (%)  CO2 (ppm)  일사 (w/㎡)\n",
              "0      2017-10-20   0:15:00    18.5    90.7      651.0       6.8\n",
              "1      2017-10-20   1:15:00    18.2    90.4      675.5       6.8\n",
              "2      2017-10-20   2:15:00    17.6    89.8      685.5       7.3\n",
              "3      2017-10-20   3:15:00    17.2    90.5      722.5       6.8\n",
              "4      2017-10-20   4:15:00    18.1    90.8      732.5       7.3\n",
              "...           ...       ...     ...     ...        ...       ...\n",
              "10171  2018-02-02  19:15:00    14.8    87.0      482.5       7.1\n",
              "10172  2018-02-02  20:15:00    15.5    85.3      492.5       7.1\n",
              "10173  2018-02-02  21:15:00    15.3    84.4      515.5       7.3\n",
              "10174  2018-02-02  22:15:00    16.0    83.4      542.0       7.3\n",
              "10175  2018-02-02  23:15:00    15.9    82.8      562.5       7.3\n",
              "\n",
              "[10176 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c753d21-429c-4092-8fcb-670dfe77f418\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>날짜</th>\n",
              "      <th>시:분</th>\n",
              "      <th>온도 (℃)</th>\n",
              "      <th>습도 (%)</th>\n",
              "      <th>CO2 (ppm)</th>\n",
              "      <th>일사 (w/㎡)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-10-20</td>\n",
              "      <td>0:15:00</td>\n",
              "      <td>18.5</td>\n",
              "      <td>90.7</td>\n",
              "      <td>651.0</td>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-10-20</td>\n",
              "      <td>1:15:00</td>\n",
              "      <td>18.2</td>\n",
              "      <td>90.4</td>\n",
              "      <td>675.5</td>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-10-20</td>\n",
              "      <td>2:15:00</td>\n",
              "      <td>17.6</td>\n",
              "      <td>89.8</td>\n",
              "      <td>685.5</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-10-20</td>\n",
              "      <td>3:15:00</td>\n",
              "      <td>17.2</td>\n",
              "      <td>90.5</td>\n",
              "      <td>722.5</td>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-10-20</td>\n",
              "      <td>4:15:00</td>\n",
              "      <td>18.1</td>\n",
              "      <td>90.8</td>\n",
              "      <td>732.5</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10171</th>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>19:15:00</td>\n",
              "      <td>14.8</td>\n",
              "      <td>87.0</td>\n",
              "      <td>482.5</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10172</th>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>20:15:00</td>\n",
              "      <td>15.5</td>\n",
              "      <td>85.3</td>\n",
              "      <td>492.5</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10173</th>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>21:15:00</td>\n",
              "      <td>15.3</td>\n",
              "      <td>84.4</td>\n",
              "      <td>515.5</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10174</th>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>22:15:00</td>\n",
              "      <td>16.0</td>\n",
              "      <td>83.4</td>\n",
              "      <td>542.0</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10175</th>\n",
              "      <td>2018-02-02</td>\n",
              "      <td>23:15:00</td>\n",
              "      <td>15.9</td>\n",
              "      <td>82.8</td>\n",
              "      <td>562.5</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10176 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c753d21-429c-4092-8fcb-670dfe77f418')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c753d21-429c-4092-8fcb-670dfe77f418 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c753d21-429c-4092-8fcb-670dfe77f418');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df1.values\n",
        "df3 = np.ravel(df3, order='F')\n",
        "df3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw1FRJe_mQvL",
        "outputId": "c50a6c2c-37b5-4b41-ec9c-ca502df2cbfa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30. , 27. , 20. , 25. , 20. ,  8. , 40. , 12. , 24. , 13. , 19. ,\n",
              "       18. , 20. , 19. , 28. , 14. , 26. , 27. , 26. , 25. , 11. , 20. ,\n",
              "       34. , 10. , 20. , 21. , 18. , 16. , 20. , 18. , 27. , 26. , 27. ,\n",
              "       26. , 20. , 24. , 20. , 43. ,  5. , 11. , 21. , 23. , 22. , 19. ,\n",
              "       22. , 25. , 19. , 24. , 32. , 26.5, 18.5, 25. , 26. , 23. , 18. ,\n",
              "       10. , 35. ,  9. , 25. , 19. , 20.5, 25.5, 21. , 21. ])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df2.drop([\"날짜\", \"시:분\"], axis=1)\n",
        "dataset = dataset.reset_index(drop=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "GOp2WZoImYW1",
        "outputId": "80635f36-d35d-4e65-8fc7-555bbde3086b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       온도 (℃)  습도 (%)  CO2 (ppm)  일사 (w/㎡)\n",
              "0        18.5    90.7      651.0       6.8\n",
              "1        18.2    90.4      675.5       6.8\n",
              "2        17.6    89.8      685.5       7.3\n",
              "3        17.2    90.5      722.5       6.8\n",
              "4        18.1    90.8      732.5       7.3\n",
              "...       ...     ...        ...       ...\n",
              "10171    14.8    87.0      482.5       7.1\n",
              "10172    15.5    85.3      492.5       7.1\n",
              "10173    15.3    84.4      515.5       7.3\n",
              "10174    16.0    83.4      542.0       7.3\n",
              "10175    15.9    82.8      562.5       7.3\n",
              "\n",
              "[10176 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb169f1e-c2e7-4551-ae84-5afe2eca1b8c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>온도 (℃)</th>\n",
              "      <th>습도 (%)</th>\n",
              "      <th>CO2 (ppm)</th>\n",
              "      <th>일사 (w/㎡)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.5</td>\n",
              "      <td>90.7</td>\n",
              "      <td>651.0</td>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18.2</td>\n",
              "      <td>90.4</td>\n",
              "      <td>675.5</td>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17.6</td>\n",
              "      <td>89.8</td>\n",
              "      <td>685.5</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17.2</td>\n",
              "      <td>90.5</td>\n",
              "      <td>722.5</td>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18.1</td>\n",
              "      <td>90.8</td>\n",
              "      <td>732.5</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10171</th>\n",
              "      <td>14.8</td>\n",
              "      <td>87.0</td>\n",
              "      <td>482.5</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10172</th>\n",
              "      <td>15.5</td>\n",
              "      <td>85.3</td>\n",
              "      <td>492.5</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10173</th>\n",
              "      <td>15.3</td>\n",
              "      <td>84.4</td>\n",
              "      <td>515.5</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10174</th>\n",
              "      <td>16.0</td>\n",
              "      <td>83.4</td>\n",
              "      <td>542.0</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10175</th>\n",
              "      <td>15.9</td>\n",
              "      <td>82.8</td>\n",
              "      <td>562.5</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10176 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb169f1e-c2e7-4551-ae84-5afe2eca1b8c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb169f1e-c2e7-4551-ae84-5afe2eca1b8c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb169f1e-c2e7-4551-ae84-5afe2eca1b8c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1.values\n",
        "df1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H5kL_-Lmd49",
        "outputId": "0c4814dc-fc70-4b75-d0b6-fbcaee272502"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[30. , 26. , 27. , 32. ],\n",
              "       [27. , 27. , 26. , 26.5],\n",
              "       [20. , 26. , 20. , 18.5],\n",
              "       [25. , 25. , 24. , 25. ],\n",
              "       [20. , 11. , 20. , 26. ],\n",
              "       [ 8. , 20. , 43. , 23. ],\n",
              "       [40. , 34. ,  5. , 18. ],\n",
              "       [12. , 10. , 11. , 10. ],\n",
              "       [24. , 20. , 21. , 35. ],\n",
              "       [13. , 21. , 23. ,  9. ],\n",
              "       [19. , 18. , 22. , 25. ],\n",
              "       [18. , 16. , 19. , 19. ],\n",
              "       [20. , 20. , 22. , 20.5],\n",
              "       [19. , 18. , 25. , 25.5],\n",
              "       [28. , 27. , 19. , 21. ],\n",
              "       [14. , 26. , 24. , 21. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset\n",
        "scaler = StandardScaler()\n",
        "dataset = scaler.fit_transform(dataset)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rbq581jmfUw",
        "outputId": "7b4bd420-b21a-4490-a186-d7c57d38662b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.04848716,  0.95551355,  0.45589463, -0.60461754],\n",
              "       [-0.02490713,  0.92769734,  0.66279392, -0.60461754],\n",
              "       [-0.17169572,  0.87206491,  0.74724261, -0.60276951],\n",
              "       ...,\n",
              "       [-0.7343853 ,  0.37137306, -0.68838513, -0.60276951],\n",
              "       [-0.56313195,  0.27865235, -0.4645961 , -0.60276951],\n",
              "       [-0.58759671,  0.22301992, -0.29147628, -0.60276951]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.reshape(64,159,4)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1905PSRbmhLz",
        "outputId": "56209479-3d2f-4d5d-b358-c5a8a52c0ef8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0.04848716  0.95551355  0.45589463 -0.60461754]\n",
            "  [-0.02490713  0.92769734  0.66279392 -0.60461754]\n",
            "  [-0.17169572  0.87206491  0.74724261 -0.60276951]\n",
            "  ...\n",
            "  [ 1.88334449 -1.80756369 -1.49909256  2.99128369]\n",
            "  [ 2.07906261 -1.95591683 -1.79888541  1.6873121 ]\n",
            "  [ 1.90780926 -2.54005732 -1.62998803  1.71614141]]\n",
            "\n",
            " [[ 1.56530256 -1.95591683 -1.84955462  0.81060558]\n",
            "  [ 1.41851397 -1.82610783 -2.10712313 -0.12893403]\n",
            "  [ 0.51331769 -0.08295843 -1.57087394 -0.53143546]\n",
            "  ...\n",
            "  [-0.07383666  0.70516763  0.71768557 -0.60276951]\n",
            "  [-0.09830142  0.74225591  0.87813808 -0.60461754]\n",
            "  [-0.17169572  0.83497662  1.08081494 -0.60535676]]\n",
            "\n",
            " [[-0.22062525  0.88133698  1.26660206 -0.60461754]\n",
            "  [-0.09830142  0.90915319  1.47350135 -0.51517278]\n",
            "  [-0.04937189  0.77007213  1.36371805 -0.19657201]\n",
            "  ...\n",
            "  [-0.58759671  0.09321092 -1.5539842  -0.60276951]\n",
            "  [-1.076892    0.13029921 -1.26263622 -0.60535676]\n",
            "  [-0.44080813  0.69589555 -0.90795172 -0.60535676]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.90563865  0.31574063  0.56567793 -0.6020303 ]\n",
            "  [-0.95456818  0.30646856  0.61043573 -0.60166069]\n",
            "  [-1.00349771  0.29719649  0.70501827 -0.60166069]\n",
            "  ...\n",
            "  [ 1.61423208 -1.46449705 -1.30063813  1.34764377]\n",
            "  [ 0.66010627 -0.45384128 -2.02267444  0.36412098]\n",
            "  [ 0.09741669  0.5197262  -1.91289114 -0.45825338]]\n",
            "\n",
            " [[-0.29401954  0.49190999 -1.50331499 -0.60350872]\n",
            "  [-0.66099101  0.41773342 -1.35552978 -0.60461754]\n",
            "  [-0.48973765  0.44554963 -1.16129779 -0.60461754]\n",
            "  ...\n",
            "  [-1.19921582  0.13029921  0.16454665 -0.60092148]\n",
            "  [-1.19921582  0.15811542  0.12654474 -0.59981266]\n",
            "  [-1.22368059  0.33428478  0.48967411 -0.43127211]]\n",
            "\n",
            " [[-0.90563865  0.54754241  0.18988125  1.1491651 ]\n",
            "  [ 1.24726062 -1.21415113  1.57483978  1.841438  ]\n",
            "  [ 1.22279585 -2.28043932 -0.8192806   1.78821467]\n",
            "  ...\n",
            "  [-0.7343853   0.37137306 -0.68838513 -0.60276951]\n",
            "  [-0.56313195  0.27865235 -0.4645961  -0.60276951]\n",
            "  [-0.58759671  0.22301992 -0.29147628 -0.60276951]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df3.reshape(64,1,1)\n",
        "print(df3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K--FPyjWmp58",
        "outputId": "d7e96903-4967-4480-bba6-74adeaa6ba10"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[30. ]]\n",
            "\n",
            " [[27. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[ 8. ]]\n",
            "\n",
            " [[40. ]]\n",
            "\n",
            " [[12. ]]\n",
            "\n",
            " [[24. ]]\n",
            "\n",
            " [[13. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[18. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[28. ]]\n",
            "\n",
            " [[14. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[27. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[11. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[34. ]]\n",
            "\n",
            " [[10. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[21. ]]\n",
            "\n",
            " [[18. ]]\n",
            "\n",
            " [[16. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[18. ]]\n",
            "\n",
            " [[27. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[27. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[24. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[43. ]]\n",
            "\n",
            " [[ 5. ]]\n",
            "\n",
            " [[11. ]]\n",
            "\n",
            " [[21. ]]\n",
            "\n",
            " [[23. ]]\n",
            "\n",
            " [[22. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[22. ]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[24. ]]\n",
            "\n",
            " [[32. ]]\n",
            "\n",
            " [[26.5]]\n",
            "\n",
            " [[18.5]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[23. ]]\n",
            "\n",
            " [[18. ]]\n",
            "\n",
            " [[10. ]]\n",
            "\n",
            " [[35. ]]\n",
            "\n",
            " [[ 9. ]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[20.5]]\n",
            "\n",
            " [[25.5]]\n",
            "\n",
            " [[21. ]]\n",
            "\n",
            " [[21. ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = dataset\n",
        "test = df3\n",
        "print(train)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSsJoc0_mspf",
        "outputId": "7de577ef-0d5a-46d6-e4ce-1290faeaecc1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0.04848716  0.95551355  0.45589463 -0.60461754]\n",
            "  [-0.02490713  0.92769734  0.66279392 -0.60461754]\n",
            "  [-0.17169572  0.87206491  0.74724261 -0.60276951]\n",
            "  ...\n",
            "  [ 1.88334449 -1.80756369 -1.49909256  2.99128369]\n",
            "  [ 2.07906261 -1.95591683 -1.79888541  1.6873121 ]\n",
            "  [ 1.90780926 -2.54005732 -1.62998803  1.71614141]]\n",
            "\n",
            " [[ 1.56530256 -1.95591683 -1.84955462  0.81060558]\n",
            "  [ 1.41851397 -1.82610783 -2.10712313 -0.12893403]\n",
            "  [ 0.51331769 -0.08295843 -1.57087394 -0.53143546]\n",
            "  ...\n",
            "  [-0.07383666  0.70516763  0.71768557 -0.60276951]\n",
            "  [-0.09830142  0.74225591  0.87813808 -0.60461754]\n",
            "  [-0.17169572  0.83497662  1.08081494 -0.60535676]]\n",
            "\n",
            " [[-0.22062525  0.88133698  1.26660206 -0.60461754]\n",
            "  [-0.09830142  0.90915319  1.47350135 -0.51517278]\n",
            "  [-0.04937189  0.77007213  1.36371805 -0.19657201]\n",
            "  ...\n",
            "  [-0.58759671  0.09321092 -1.5539842  -0.60276951]\n",
            "  [-1.076892    0.13029921 -1.26263622 -0.60535676]\n",
            "  [-0.44080813  0.69589555 -0.90795172 -0.60535676]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.90563865  0.31574063  0.56567793 -0.6020303 ]\n",
            "  [-0.95456818  0.30646856  0.61043573 -0.60166069]\n",
            "  [-1.00349771  0.29719649  0.70501827 -0.60166069]\n",
            "  ...\n",
            "  [ 1.61423208 -1.46449705 -1.30063813  1.34764377]\n",
            "  [ 0.66010627 -0.45384128 -2.02267444  0.36412098]\n",
            "  [ 0.09741669  0.5197262  -1.91289114 -0.45825338]]\n",
            "\n",
            " [[-0.29401954  0.49190999 -1.50331499 -0.60350872]\n",
            "  [-0.66099101  0.41773342 -1.35552978 -0.60461754]\n",
            "  [-0.48973765  0.44554963 -1.16129779 -0.60461754]\n",
            "  ...\n",
            "  [-1.19921582  0.13029921  0.16454665 -0.60092148]\n",
            "  [-1.19921582  0.15811542  0.12654474 -0.59981266]\n",
            "  [-1.22368059  0.33428478  0.48967411 -0.43127211]]\n",
            "\n",
            " [[-0.90563865  0.54754241  0.18988125  1.1491651 ]\n",
            "  [ 1.24726062 -1.21415113  1.57483978  1.841438  ]\n",
            "  [ 1.22279585 -2.28043932 -0.8192806   1.78821467]\n",
            "  ...\n",
            "  [-0.7343853   0.37137306 -0.68838513 -0.60276951]\n",
            "  [-0.56313195  0.27865235 -0.4645961  -0.60276951]\n",
            "  [-0.58759671  0.22301992 -0.29147628 -0.60276951]]]\n",
            "[[[30. ]]\n",
            "\n",
            " [[27. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[ 8. ]]\n",
            "\n",
            " [[40. ]]\n",
            "\n",
            " [[12. ]]\n",
            "\n",
            " [[24. ]]\n",
            "\n",
            " [[13. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[18. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[28. ]]\n",
            "\n",
            " [[14. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[27. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[11. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[34. ]]\n",
            "\n",
            " [[10. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[21. ]]\n",
            "\n",
            " [[18. ]]\n",
            "\n",
            " [[16. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[18. ]]\n",
            "\n",
            " [[27. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[27. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[24. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[43. ]]\n",
            "\n",
            " [[ 5. ]]\n",
            "\n",
            " [[11. ]]\n",
            "\n",
            " [[21. ]]\n",
            "\n",
            " [[23. ]]\n",
            "\n",
            " [[22. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[22. ]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[24. ]]\n",
            "\n",
            " [[32. ]]\n",
            "\n",
            " [[26.5]]\n",
            "\n",
            " [[18.5]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[23. ]]\n",
            "\n",
            " [[18. ]]\n",
            "\n",
            " [[10. ]]\n",
            "\n",
            " [[35. ]]\n",
            "\n",
            " [[ 9. ]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[20.5]]\n",
            "\n",
            " [[25.5]]\n",
            "\n",
            " [[21. ]]\n",
            "\n",
            " [[21. ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split into input and outputs\n",
        "train_X, test_X = train[:50], train[50:]\n",
        "train_y, test_y = test[:50], test[50:]\n",
        "\n",
        "print(train_X, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU7-_9p6m9IP",
        "outputId": "de52916a-5d47-4a27-f7c4-478bf12624c0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0.04848716  0.95551355  0.45589463 -0.60461754]\n",
            "  [-0.02490713  0.92769734  0.66279392 -0.60461754]\n",
            "  [-0.17169572  0.87206491  0.74724261 -0.60276951]\n",
            "  ...\n",
            "  [ 1.88334449 -1.80756369 -1.49909256  2.99128369]\n",
            "  [ 2.07906261 -1.95591683 -1.79888541  1.6873121 ]\n",
            "  [ 1.90780926 -2.54005732 -1.62998803  1.71614141]]\n",
            "\n",
            " [[ 1.56530256 -1.95591683 -1.84955462  0.81060558]\n",
            "  [ 1.41851397 -1.82610783 -2.10712313 -0.12893403]\n",
            "  [ 0.51331769 -0.08295843 -1.57087394 -0.53143546]\n",
            "  ...\n",
            "  [-0.07383666  0.70516763  0.71768557 -0.60276951]\n",
            "  [-0.09830142  0.74225591  0.87813808 -0.60461754]\n",
            "  [-0.17169572  0.83497662  1.08081494 -0.60535676]]\n",
            "\n",
            " [[-0.22062525  0.88133698  1.26660206 -0.60461754]\n",
            "  [-0.09830142  0.90915319  1.47350135 -0.51517278]\n",
            "  [-0.04937189  0.77007213  1.36371805 -0.19657201]\n",
            "  ...\n",
            "  [-0.58759671  0.09321092 -1.5539842  -0.60276951]\n",
            "  [-1.076892    0.13029921 -1.26263622 -0.60535676]\n",
            "  [-0.44080813  0.69589555 -0.90795172 -0.60535676]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.90563865  0.54754241  0.18988125  1.1491651 ]\n",
            "  [ 1.24726062 -1.21415113  1.57483978  1.841438  ]\n",
            "  [ 1.22279585 -2.28043932 -0.8192806   1.78821467]\n",
            "  ...\n",
            "  [-0.7343853   0.37137306 -0.68838513 -0.60276951]\n",
            "  [-0.56313195  0.27865235 -0.4645961  -0.60276951]\n",
            "  [-0.58759671  0.22301992 -0.29147628 -0.60276951]]\n",
            "\n",
            " [[ 0.04848716  0.95551355  0.45589463 -0.60461754]\n",
            "  [-0.02490713  0.92769734  0.66279392 -0.60461754]\n",
            "  [-0.17169572  0.87206491  0.74724261 -0.60276951]\n",
            "  ...\n",
            "  [ 1.88334449 -1.80756369 -1.49909256  2.99128369]\n",
            "  [ 2.07906261 -1.95591683 -1.79888541  1.6873121 ]\n",
            "  [ 1.90780926 -2.54005732 -1.62998803  1.71614141]]\n",
            "\n",
            " [[ 1.56530256 -1.95591683 -1.84955462  0.81060558]\n",
            "  [ 1.41851397 -1.82610783 -2.10712313 -0.12893403]\n",
            "  [ 0.51331769 -0.08295843 -1.57087394 -0.53143546]\n",
            "  ...\n",
            "  [-0.07383666  0.70516763  0.71768557 -0.60276951]\n",
            "  [-0.09830142  0.74225591  0.87813808 -0.60461754]\n",
            "  [-0.17169572  0.83497662  1.08081494 -0.60535676]]] [[[30. ]]\n",
            "\n",
            " [[27. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[ 8. ]]\n",
            "\n",
            " [[40. ]]\n",
            "\n",
            " [[12. ]]\n",
            "\n",
            " [[24. ]]\n",
            "\n",
            " [[13. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[18. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[28. ]]\n",
            "\n",
            " [[14. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[27. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[11. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[34. ]]\n",
            "\n",
            " [[10. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[21. ]]\n",
            "\n",
            " [[18. ]]\n",
            "\n",
            " [[16. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[18. ]]\n",
            "\n",
            " [[27. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[27. ]]\n",
            "\n",
            " [[26. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[24. ]]\n",
            "\n",
            " [[20. ]]\n",
            "\n",
            " [[43. ]]\n",
            "\n",
            " [[ 5. ]]\n",
            "\n",
            " [[11. ]]\n",
            "\n",
            " [[21. ]]\n",
            "\n",
            " [[23. ]]\n",
            "\n",
            " [[22. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[22. ]]\n",
            "\n",
            " [[25. ]]\n",
            "\n",
            " [[19. ]]\n",
            "\n",
            " [[24. ]]\n",
            "\n",
            " [[32. ]]\n",
            "\n",
            " [[26.5]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape input to be 3D [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], train_X.shape[1],4))\n",
        "test_X = test_X.reshape((test_X.shape[0], test_X.shape[1], 4))\n",
        "train_y = train_y.reshape((train_y.shape[0], train_y.shape[1],1))\n",
        "test_y = test_y.reshape((test_y.shape[0], test_y.shape[1],1))\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7674fWzmnBJ3",
        "outputId": "4cbd10aa-6c52-452d-eca9-2496b8b7cd19"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 159, 4) (50, 1, 1) (14, 159, 4) (14, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X\n",
        "train_y\n",
        "test_X\n",
        "test_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eddcNr12nDMW",
        "outputId": "deb22614-abe0-4797-b8d8-cbede6f7cb7c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[18.5]],\n",
              "\n",
              "       [[25. ]],\n",
              "\n",
              "       [[26. ]],\n",
              "\n",
              "       [[23. ]],\n",
              "\n",
              "       [[18. ]],\n",
              "\n",
              "       [[10. ]],\n",
              "\n",
              "       [[35. ]],\n",
              "\n",
              "       [[ 9. ]],\n",
              "\n",
              "       [[25. ]],\n",
              "\n",
              "       [[19. ]],\n",
              "\n",
              "       [[20.5]],\n",
              "\n",
              "       [[25.5]],\n",
              "\n",
              "       [[21. ]],\n",
              "\n",
              "       [[21. ]]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads\n",
        "        # , dropout=dropout\n",
        "    )(x, x)\n",
        "    # x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    # x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    # x = layers.Dropout(dropout)(x)\n",
        "    # x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ],
      "metadata": {
        "id": "TagNmkeUnEUI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    # x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    # for dim in mlp_units:\n",
        "    #     x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "    #     x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1, activation=\"relu\")(x)\n",
        "    return keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "D0digrNenFvs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = train_X.shape[1:]\n",
        "print(input_shape)\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.MeanSquaredError(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=[\"mse\"],\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "\n",
        "model.fit(\n",
        "    train_X,\n",
        "    train_y,\n",
        "    validation_split=0.2,\n",
        "    epochs=200,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNcIZAHanG4W",
        "outputId": "1d5a7cde-8316-4ec4-fb01-3902576262b6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(159, 4)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 159, 4)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 159, 4)      8           ['input_1[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 159, 4)      19460       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 159, 4)      0           ['multi_head_attention[0][0]',   \n",
            " da)                                                              'input_1[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 159, 4)      8           ['tf.__operators__.add[0][0]']   \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 159, 4)      0           ['layer_normalization_1[0][0]',  \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 159, 4)      8           ['tf.__operators__.add_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 159, 4)      19460       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 159, 4)      0           ['multi_head_attention_1[0][0]', \n",
            " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 159, 4)      8           ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 159, 4)      0           ['layer_normalization_3[0][0]',  \n",
            " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 159, 4)      8           ['tf.__operators__.add_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 159, 4)      19460       ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 159, 4)      0           ['multi_head_attention_2[0][0]', \n",
            " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 159, 4)      8           ['tf.__operators__.add_4[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 159, 4)      0           ['layer_normalization_5[0][0]',  \n",
            " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 159, 4)      8           ['tf.__operators__.add_5[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 159, 4)      19460       ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 159, 4)      0           ['multi_head_attention_3[0][0]', \n",
            " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 159, 4)      8           ['tf.__operators__.add_6[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 159, 4)      0           ['layer_normalization_7[0][0]',  \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 159, 1)       5           ['tf.__operators__.add_7[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 77,909\n",
            "Trainable params: 77,909\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "1/1 [==============================] - 17s 17s/step - loss: 478.9049 - mse: 478.9049 - val_loss: 510.0906 - val_mse: 510.0906\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 477.8963 - mse: 477.8963 - val_loss: 508.9477 - val_mse: 508.9477\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 476.8753 - mse: 476.8753 - val_loss: 507.7899 - val_mse: 507.7899\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 475.8499 - mse: 475.8499 - val_loss: 506.6183 - val_mse: 506.6183\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 474.8214 - mse: 474.8213 - val_loss: 505.4462 - val_mse: 505.4462\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 473.7931 - mse: 473.7931 - val_loss: 504.2714 - val_mse: 504.2714\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 472.7624 - mse: 472.7624 - val_loss: 503.0819 - val_mse: 503.0819\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 471.7188 - mse: 471.7188 - val_loss: 501.8417 - val_mse: 501.8417\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 470.6517 - mse: 470.6516 - val_loss: 500.5515 - val_mse: 500.5515\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 469.5421 - mse: 469.5421 - val_loss: 499.1972 - val_mse: 499.1972\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 468.3924 - mse: 468.3924 - val_loss: 497.7455 - val_mse: 497.7455\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 467.1778 - mse: 467.1779 - val_loss: 496.1471 - val_mse: 496.1471\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 465.8658 - mse: 465.8658 - val_loss: 494.3675 - val_mse: 494.3675\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 464.4360 - mse: 464.4360 - val_loss: 492.2792 - val_mse: 492.2792\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 462.8111 - mse: 462.8111 - val_loss: 489.8472 - val_mse: 489.8472\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 460.9745 - mse: 460.9745 - val_loss: 487.0516 - val_mse: 487.0516\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 458.8715 - mse: 458.8716 - val_loss: 483.8846 - val_mse: 483.8846\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 456.4608 - mse: 456.4608 - val_loss: 480.2583 - val_mse: 480.2583\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 453.6982 - mse: 453.6982 - val_loss: 476.1432 - val_mse: 476.1432\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 450.5579 - mse: 450.5579 - val_loss: 471.6458 - val_mse: 471.6458\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 447.0981 - mse: 447.0981 - val_loss: 466.7721 - val_mse: 466.7721\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 443.3154 - mse: 443.3154 - val_loss: 461.5183 - val_mse: 461.5183\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 439.2571 - mse: 439.2571 - val_loss: 455.9487 - val_mse: 455.9487\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 434.8825 - mse: 434.8825 - val_loss: 450.0700 - val_mse: 450.0700\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 430.2167 - mse: 430.2167 - val_loss: 443.8825 - val_mse: 443.8825\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 425.2700 - mse: 425.2700 - val_loss: 437.4708 - val_mse: 437.4708\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 420.0910 - mse: 420.0910 - val_loss: 430.8369 - val_mse: 430.8369\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 414.6862 - mse: 414.6862 - val_loss: 424.0568 - val_mse: 424.0569\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 409.0896 - mse: 409.0896 - val_loss: 417.1625 - val_mse: 417.1625\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 403.3496 - mse: 403.3496 - val_loss: 410.1547 - val_mse: 410.1548\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 397.4765 - mse: 397.4765 - val_loss: 403.0678 - val_mse: 403.0678\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 391.5014 - mse: 391.5014 - val_loss: 395.9676 - val_mse: 395.9676\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 385.4656 - mse: 385.4656 - val_loss: 388.8637 - val_mse: 388.8637\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 379.3819 - mse: 379.3819 - val_loss: 381.7501 - val_mse: 381.7501\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 373.2598 - mse: 373.2598 - val_loss: 374.6705 - val_mse: 374.6705\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 367.1175 - mse: 367.1175 - val_loss: 367.6408 - val_mse: 367.6408\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 360.9741 - mse: 360.9741 - val_loss: 360.6545 - val_mse: 360.6545\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 354.8340 - mse: 354.8340 - val_loss: 353.7141 - val_mse: 353.7141\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 348.6933 - mse: 348.6933 - val_loss: 346.8246 - val_mse: 346.8246\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 342.5732 - mse: 342.5732 - val_loss: 340.0061 - val_mse: 340.0061\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 336.4790 - mse: 336.4790 - val_loss: 333.2658 - val_mse: 333.2658\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 330.4169 - mse: 330.4169 - val_loss: 326.6178 - val_mse: 326.6178\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 324.4012 - mse: 324.4012 - val_loss: 320.0564 - val_mse: 320.0564\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 318.4168 - mse: 318.4168 - val_loss: 313.5620 - val_mse: 313.5620\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 312.4651 - mse: 312.4651 - val_loss: 307.1515 - val_mse: 307.1515\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 306.5810 - mse: 306.5810 - val_loss: 300.8333 - val_mse: 300.8333\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 300.7738 - mse: 300.7738 - val_loss: 294.6055 - val_mse: 294.6055\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 295.0408 - mse: 295.0408 - val_loss: 288.4710 - val_mse: 288.4710\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 289.3845 - mse: 289.3845 - val_loss: 282.4247 - val_mse: 282.4247\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 283.8092 - mse: 283.8092 - val_loss: 276.4711 - val_mse: 276.4711\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 278.3194 - mse: 278.3194 - val_loss: 270.6017 - val_mse: 270.6017\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 272.9124 - mse: 272.9124 - val_loss: 264.8368 - val_mse: 264.8368\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 267.5978 - mse: 267.5978 - val_loss: 259.1751 - val_mse: 259.1751\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 262.3755 - mse: 262.3755 - val_loss: 253.6159 - val_mse: 253.6159\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 257.2491 - mse: 257.2491 - val_loss: 248.1582 - val_mse: 248.1582\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 252.2185 - mse: 252.2185 - val_loss: 242.8008 - val_mse: 242.8008\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 247.2837 - mse: 247.2837 - val_loss: 237.5352 - val_mse: 237.5352\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 242.4411 - mse: 242.4411 - val_loss: 232.3670 - val_mse: 232.3670\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 237.6929 - mse: 237.6929 - val_loss: 227.2953 - val_mse: 227.2953\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 233.0387 - mse: 233.0387 - val_loss: 222.3189 - val_mse: 222.3189\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 228.4778 - mse: 228.4778 - val_loss: 217.4366 - val_mse: 217.4366\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 224.0094 - mse: 224.0094 - val_loss: 212.6448 - val_mse: 212.6448\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 219.6316 - mse: 219.6316 - val_loss: 207.9392 - val_mse: 207.9392\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 215.3424 - mse: 215.3424 - val_loss: 203.3237 - val_mse: 203.3237\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 211.1428 - mse: 211.1428 - val_loss: 198.7972 - val_mse: 198.7972\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 207.0318 - mse: 207.0318 - val_loss: 194.3585 - val_mse: 194.3585\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 203.0086 - mse: 203.0086 - val_loss: 190.0063 - val_mse: 190.0063\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 199.0720 - mse: 199.0720 - val_loss: 185.7395 - val_mse: 185.7395\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 195.2209 - mse: 195.2209 - val_loss: 181.5569 - val_mse: 181.5569\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 191.4545 - mse: 191.4545 - val_loss: 177.4574 - val_mse: 177.4574\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 187.7713 - mse: 187.7713 - val_loss: 173.4398 - val_mse: 173.4398\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 184.1705 - mse: 184.1705 - val_loss: 169.5029 - val_mse: 169.5029\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 180.6508 - mse: 180.6508 - val_loss: 165.6457 - val_mse: 165.6457\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 177.2111 - mse: 177.2111 - val_loss: 161.8669 - val_mse: 161.8669\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 173.8502 - mse: 173.8502 - val_loss: 158.1655 - val_mse: 158.1655\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 170.5670 - mse: 170.5670 - val_loss: 154.5405 - val_mse: 154.5405\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 167.3603 - mse: 167.3603 - val_loss: 150.9906 - val_mse: 150.9906\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 164.2290 - mse: 164.2290 - val_loss: 147.5148 - val_mse: 147.5148\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 161.1719 - mse: 161.1719 - val_loss: 144.1121 - val_mse: 144.1121\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 158.1878 - mse: 158.1878 - val_loss: 140.7814 - val_mse: 140.7814\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 155.2757 - mse: 155.2757 - val_loss: 137.5217 - val_mse: 137.5217\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 152.4342 - mse: 152.4342 - val_loss: 134.3319 - val_mse: 134.3319\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 149.6624 - mse: 149.6624 - val_loss: 131.2111 - val_mse: 131.2111\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 146.9591 - mse: 146.9591 - val_loss: 128.1582 - val_mse: 128.1582\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 144.3233 - mse: 144.3233 - val_loss: 125.1723 - val_mse: 125.1723\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 141.7537 - mse: 141.7537 - val_loss: 122.2525 - val_mse: 122.2525\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 139.2493 - mse: 139.2493 - val_loss: 119.3977 - val_mse: 119.3977\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 136.8090 - mse: 136.8090 - val_loss: 116.6070 - val_mse: 116.6070\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 134.4318 - mse: 134.4318 - val_loss: 113.8795 - val_mse: 113.8795\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 132.1167 - mse: 132.1167 - val_loss: 111.2142 - val_mse: 111.2142\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 129.8624 - mse: 129.8624 - val_loss: 108.6104 - val_mse: 108.6104\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 127.6682 - mse: 127.6682 - val_loss: 106.0670 - val_mse: 106.0670\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 125.5328 - mse: 125.5328 - val_loss: 103.5832 - val_mse: 103.5832\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 123.4554 - mse: 123.4554 - val_loss: 101.1581 - val_mse: 101.1581\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 121.4350 - mse: 121.4350 - val_loss: 98.7908 - val_mse: 98.7908\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 119.4705 - mse: 119.4705 - val_loss: 96.4805 - val_mse: 96.4805\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 117.5610 - mse: 117.5610 - val_loss: 94.2263 - val_mse: 94.2263\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 115.7055 - mse: 115.7055 - val_loss: 92.0274 - val_mse: 92.0274\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 113.9031 - mse: 113.9031 - val_loss: 89.8830 - val_mse: 89.8830\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 112.1528 - mse: 112.1528 - val_loss: 87.7922 - val_mse: 87.7922\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 110.4537 - mse: 110.4537 - val_loss: 85.7542 - val_mse: 85.7542\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 108.8050 - mse: 108.8050 - val_loss: 83.7682 - val_mse: 83.7682\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 107.2056 - mse: 107.2056 - val_loss: 81.8333 - val_mse: 81.8333\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 105.6546 - mse: 105.6546 - val_loss: 79.9488 - val_mse: 79.9488\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 104.1512 - mse: 104.1512 - val_loss: 78.1138 - val_mse: 78.1138\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 102.6944 - mse: 102.6945 - val_loss: 76.3276 - val_mse: 76.3276\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 101.2835 - mse: 101.2835 - val_loss: 74.5894 - val_mse: 74.5894\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 99.9174 - mse: 99.9174 - val_loss: 72.8983 - val_mse: 72.8983\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 98.5953 - mse: 98.5953 - val_loss: 71.2536 - val_mse: 71.2536\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 97.3163 - mse: 97.3163 - val_loss: 69.6544 - val_mse: 69.6544\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 96.0795 - mse: 96.0795 - val_loss: 68.1001 - val_mse: 68.1001\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 94.8842 - mse: 94.8842 - val_loss: 66.5897 - val_mse: 66.5897\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 93.7293 - mse: 93.7293 - val_loss: 65.1226 - val_mse: 65.1226\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 92.6141 - mse: 92.6141 - val_loss: 63.6978 - val_mse: 63.6978\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 91.5376 - mse: 91.5376 - val_loss: 62.3147 - val_mse: 62.3147\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 90.4991 - mse: 90.4991 - val_loss: 60.9725 - val_mse: 60.9725\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 89.4977 - mse: 89.4977 - val_loss: 59.6703 - val_mse: 59.6703\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 88.5324 - mse: 88.5324 - val_loss: 58.4074 - val_mse: 58.4074\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 87.6026 - mse: 87.6026 - val_loss: 57.1831 - val_mse: 57.1831\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 86.7073 - mse: 86.7073 - val_loss: 55.9964 - val_mse: 55.9964\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 85.8456 - mse: 85.8456 - val_loss: 54.8468 - val_mse: 54.8468\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 85.0168 - mse: 85.0168 - val_loss: 53.7332 - val_mse: 53.7332\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 84.2200 - mse: 84.2200 - val_loss: 52.6551 - val_mse: 52.6551\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 83.4543 - mse: 83.4543 - val_loss: 51.6116 - val_mse: 51.6116\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 82.7190 - mse: 82.7190 - val_loss: 50.6020 - val_mse: 50.6020\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 82.0132 - mse: 82.0132 - val_loss: 49.6254 - val_mse: 49.6254\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 81.3362 - mse: 81.3362 - val_loss: 48.6812 - val_mse: 48.6812\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 80.6870 - mse: 80.6870 - val_loss: 47.7685 - val_mse: 47.7685\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 80.0648 - mse: 80.0648 - val_loss: 46.8866 - val_mse: 46.8866\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 79.4690 - mse: 79.4690 - val_loss: 46.0347 - val_mse: 46.0347\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 78.8986 - mse: 78.8986 - val_loss: 45.2121 - val_mse: 45.2121\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 78.3529 - mse: 78.3529 - val_loss: 44.4181 - val_mse: 44.4181\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 77.8311 - mse: 77.8311 - val_loss: 43.6518 - val_mse: 43.6518\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 77.3325 - mse: 77.3325 - val_loss: 42.9126 - val_mse: 42.9126\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 76.8562 - mse: 76.8562 - val_loss: 42.1998 - val_mse: 42.1998\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 76.4015 - mse: 76.4015 - val_loss: 41.5125 - val_mse: 41.5125\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 75.9677 - mse: 75.9677 - val_loss: 40.8501 - val_mse: 40.8501\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 75.5539 - mse: 75.5539 - val_loss: 40.2119 - val_mse: 40.2119\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 75.1596 - mse: 75.1596 - val_loss: 39.5972 - val_mse: 39.5972\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 74.7840 - mse: 74.7840 - val_loss: 39.0053 - val_mse: 39.0053\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 74.4263 - mse: 74.4263 - val_loss: 38.4354 - val_mse: 38.4354\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 74.0859 - mse: 74.0859 - val_loss: 37.8870 - val_mse: 37.8870\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 73.7621 - mse: 73.7621 - val_loss: 37.3593 - val_mse: 37.3593\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 73.4543 - mse: 73.4543 - val_loss: 36.8518 - val_mse: 36.8518\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 73.1617 - mse: 73.1617 - val_loss: 36.3637 - val_mse: 36.3637\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 72.8837 - mse: 72.8837 - val_loss: 35.8944 - val_mse: 35.8944\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 72.6198 - mse: 72.6198 - val_loss: 35.4433 - val_mse: 35.4433\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 72.3692 - mse: 72.3692 - val_loss: 35.0098 - val_mse: 35.0098\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 72.1314 - mse: 72.1314 - val_loss: 34.5933 - val_mse: 34.5933\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 71.9058 - mse: 71.9058 - val_loss: 34.1933 - val_mse: 34.1933\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 71.6919 - mse: 71.6919 - val_loss: 33.8090 - val_mse: 33.8090\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 71.4891 - mse: 71.4891 - val_loss: 33.4401 - val_mse: 33.4401\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 71.2968 - mse: 71.2968 - val_loss: 33.0858 - val_mse: 33.0858\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 71.1146 - mse: 71.1146 - val_loss: 32.7458 - val_mse: 32.7458\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 70.9418 - mse: 70.9418 - val_loss: 32.4194 - val_mse: 32.4194\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 70.7782 - mse: 70.7782 - val_loss: 32.1062 - val_mse: 32.1062\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 70.6230 - mse: 70.6230 - val_loss: 31.8057 - val_mse: 31.8057\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 70.4760 - mse: 70.4760 - val_loss: 31.5174 - val_mse: 31.5174\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 70.3367 - mse: 70.3367 - val_loss: 31.2408 - val_mse: 31.2408\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 70.2046 - mse: 70.2046 - val_loss: 30.9754 - val_mse: 30.9754\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 70.0793 - mse: 70.0793 - val_loss: 30.7209 - val_mse: 30.7209\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 69.9605 - mse: 69.9605 - val_loss: 30.4768 - val_mse: 30.4768\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 69.8477 - mse: 69.8477 - val_loss: 30.2427 - val_mse: 30.2427\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 69.7407 - mse: 69.7407 - val_loss: 30.0181 - val_mse: 30.0181\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 69.6390 - mse: 69.6390 - val_loss: 29.8028 - val_mse: 29.8028\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 69.5423 - mse: 69.5423 - val_loss: 29.5963 - val_mse: 29.5963\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 69.4503 - mse: 69.4503 - val_loss: 29.3982 - val_mse: 29.3982\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 69.3628 - mse: 69.3628 - val_loss: 29.2083 - val_mse: 29.2083\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 69.2793 - mse: 69.2793 - val_loss: 29.0261 - val_mse: 29.0261\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 69.1997 - mse: 69.1997 - val_loss: 28.8514 - val_mse: 28.8514\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 69.1237 - mse: 69.1237 - val_loss: 28.6839 - val_mse: 28.6839\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 69.0511 - mse: 69.0511 - val_loss: 28.5232 - val_mse: 28.5232\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 68.9815 - mse: 68.9815 - val_loss: 28.3690 - val_mse: 28.3690\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 68.9149 - mse: 68.9149 - val_loss: 28.2211 - val_mse: 28.2211\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 68.8509 - mse: 68.8509 - val_loss: 28.0792 - val_mse: 28.0792\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 68.7894 - mse: 68.7894 - val_loss: 27.9431 - val_mse: 27.9431\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 68.7302 - mse: 68.7302 - val_loss: 27.8125 - val_mse: 27.8125\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 68.6731 - mse: 68.6731 - val_loss: 27.6872 - val_mse: 27.6872\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 68.6180 - mse: 68.6180 - val_loss: 27.5668 - val_mse: 27.5668\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 68.5646 - mse: 68.5646 - val_loss: 27.4514 - val_mse: 27.4514\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 68.5130 - mse: 68.5130 - val_loss: 27.3405 - val_mse: 27.3405\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 68.4628 - mse: 68.4628 - val_loss: 27.2340 - val_mse: 27.2340\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 68.4141 - mse: 68.4141 - val_loss: 27.1318 - val_mse: 27.1318\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 68.3666 - mse: 68.3666 - val_loss: 27.0336 - val_mse: 27.0336\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 68.3203 - mse: 68.3203 - val_loss: 26.9392 - val_mse: 26.9392\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 68.2751 - mse: 68.2751 - val_loss: 26.8485 - val_mse: 26.8485\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 68.2308 - mse: 68.2308 - val_loss: 26.7614 - val_mse: 26.7614\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 68.1874 - mse: 68.1874 - val_loss: 26.6777 - val_mse: 26.6777\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 68.1448 - mse: 68.1448 - val_loss: 26.5971 - val_mse: 26.5971\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 68.1030 - mse: 68.1030 - val_loss: 26.5197 - val_mse: 26.5197\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 68.0618 - mse: 68.0618 - val_loss: 26.4452 - val_mse: 26.4452\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 68.0212 - mse: 68.0212 - val_loss: 26.3735 - val_mse: 26.3735\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 67.9811 - mse: 67.9811 - val_loss: 26.3046 - val_mse: 26.3046\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 67.9415 - mse: 67.9415 - val_loss: 26.2382 - val_mse: 26.2382\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 67.9023 - mse: 67.9023 - val_loss: 26.1742 - val_mse: 26.1742\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 67.8634 - mse: 67.8634 - val_loss: 26.1126 - val_mse: 26.1126\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 67.8250 - mse: 67.8250 - val_loss: 26.0533 - val_mse: 26.0533\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 67.7868 - mse: 67.7868 - val_loss: 25.9961 - val_mse: 25.9961\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 67.7489 - mse: 67.7489 - val_loss: 25.9409 - val_mse: 25.9409\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 67.7112 - mse: 67.7112 - val_loss: 25.8877 - val_mse: 25.8877\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faf7c1950f0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_and_metrics = model.evaluate(test_X, test_y, batch_size=32)\n",
        "print('## evaluation loss and_metrics ##')\n",
        "print(loss_and_metrics)\n",
        "\n",
        "x_pred = test_X[:1]\n",
        "y_pred = model.predict(x_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF2s3H6XnH0R",
        "outputId": "7210af26-0517-46ce-aeda-2fd97ca5b0af"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step - loss: 44.8043 - mse: 44.8043\n",
            "## evaluation loss and_metrics ##\n",
            "[44.804325103759766, 44.804325103759766]\n",
            "1/1 [==============================] - 0s 341ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mrlWlfsBnJaJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'transformer_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "result = loaded_model.predict(x_pred)\n",
        "mse = np.mean((result - test_y) ** 2)\n",
        "print(result)\n",
        "print(\"Mean Squared Error (MSE):\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9btxe9xzQnf",
        "outputId": "3700e1ca-a7a5-4bb0-8fed-6daa0b1dcaa2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7faefc25d480> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 302ms/step\n",
            "[[[18.408949]\n",
            "  [18.293818]\n",
            "  [18.455946]\n",
            "  [19.754808]\n",
            "  [21.472006]\n",
            "  [21.956722]\n",
            "  [19.96176 ]\n",
            "  [21.076118]\n",
            "  [20.91165 ]\n",
            "  [21.168537]\n",
            "  [20.927942]\n",
            "  [21.632166]\n",
            "  [21.120867]\n",
            "  [20.19845 ]\n",
            "  [19.660269]\n",
            "  [19.672169]\n",
            "  [19.528414]\n",
            "  [19.371357]\n",
            "  [19.498705]\n",
            "  [19.434908]\n",
            "  [19.331442]\n",
            "  [19.088198]\n",
            "  [18.840895]\n",
            "  [18.553255]\n",
            "  [18.477371]\n",
            "  [18.577633]\n",
            "  [18.772036]\n",
            "  [20.935188]\n",
            "  [20.171202]\n",
            "  [21.333296]\n",
            "  [21.389807]\n",
            "  [21.174744]\n",
            "  [20.279531]\n",
            "  [20.632433]\n",
            "  [21.447344]\n",
            "  [21.480402]\n",
            "  [20.975632]\n",
            "  [19.519068]\n",
            "  [19.502253]\n",
            "  [19.595129]\n",
            "  [19.511503]\n",
            "  [19.167276]\n",
            "  [19.407598]\n",
            "  [19.667688]\n",
            "  [19.57688 ]\n",
            "  [19.184835]\n",
            "  [19.104082]\n",
            "  [19.226084]\n",
            "  [19.058336]\n",
            "  [19.107512]\n",
            "  [19.360506]\n",
            "  [20.588228]\n",
            "  [21.866125]\n",
            "  [22.361906]\n",
            "  [20.987827]\n",
            "  [20.945427]\n",
            "  [20.328146]\n",
            "  [21.135508]\n",
            "  [21.413727]\n",
            "  [22.039164]\n",
            "  [21.203299]\n",
            "  [19.803797]\n",
            "  [19.64079 ]\n",
            "  [20.260454]\n",
            "  [19.933435]\n",
            "  [20.09318 ]\n",
            "  [20.274965]\n",
            "  [20.261858]\n",
            "  [20.09313 ]\n",
            "  [19.849552]\n",
            "  [19.45059 ]\n",
            "  [19.19216 ]\n",
            "  [19.108458]\n",
            "  [18.700085]\n",
            "  [19.347784]\n",
            "  [19.862066]\n",
            "  [22.042786]\n",
            "  [23.358679]\n",
            "  [21.317087]\n",
            "  [21.254644]\n",
            "  [20.691963]\n",
            "  [20.626648]\n",
            "  [21.120703]\n",
            "  [22.150959]\n",
            "  [21.443398]\n",
            "  [19.725409]\n",
            "  [19.79888 ]\n",
            "  [20.198095]\n",
            "  [19.73895 ]\n",
            "  [20.020727]\n",
            "  [20.223738]\n",
            "  [20.192268]\n",
            "  [19.963614]\n",
            "  [19.837097]\n",
            "  [19.503067]\n",
            "  [19.135748]\n",
            "  [18.854061]\n",
            "  [18.602406]\n",
            "  [19.280416]\n",
            "  [19.69523 ]\n",
            "  [22.747335]\n",
            "  [23.33553 ]\n",
            "  [20.704939]\n",
            "  [22.180553]\n",
            "  [21.777903]\n",
            "  [21.342165]\n",
            "  [21.52514 ]\n",
            "  [22.127699]\n",
            "  [21.406454]\n",
            "  [19.876345]\n",
            "  [19.290476]\n",
            "  [19.467121]\n",
            "  [19.221474]\n",
            "  [19.040083]\n",
            "  [19.390495]\n",
            "  [19.22289 ]\n",
            "  [19.154396]\n",
            "  [18.975073]\n",
            "  [18.8589  ]\n",
            "  [18.661703]\n",
            "  [18.38164 ]\n",
            "  [18.291582]\n",
            "  [18.634718]\n",
            "  [21.371313]\n",
            "  [21.504198]\n",
            "  [22.25615 ]\n",
            "  [20.37035 ]\n",
            "  [21.897951]\n",
            "  [21.070257]\n",
            "  [21.306707]\n",
            "  [21.227564]\n",
            "  [21.00294 ]\n",
            "  [20.546364]\n",
            "  [20.028801]\n",
            "  [19.641062]\n",
            "  [19.501337]\n",
            "  [19.491787]\n",
            "  [19.189646]\n",
            "  [19.075733]\n",
            "  [18.832394]\n",
            "  [18.588198]\n",
            "  [18.486712]\n",
            "  [18.27902 ]\n",
            "  [18.176191]\n",
            "  [17.944117]\n",
            "  [17.895952]\n",
            "  [18.371489]\n",
            "  [21.123224]\n",
            "  [22.219337]\n",
            "  [23.096542]\n",
            "  [22.483019]\n",
            "  [21.481691]\n",
            "  [20.654083]\n",
            "  [20.829586]\n",
            "  [21.650557]\n",
            "  [21.880102]\n",
            "  [21.192133]\n",
            "  [20.684877]\n",
            "  [19.948534]]]\n",
            "Mean Squared Error (MSE): 42.55734567460175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H-fFzzDa7eEs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}